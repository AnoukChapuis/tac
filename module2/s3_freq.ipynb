{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analalyse de la distribution du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/anoukchapuis/tac/tac_venv/lib/python3.10/site-packages/regex/_regex.cpython-310-darwin.so, 0x0002): tried: '/Users/anoukchapuis/tac/tac_venv/lib/python3.10/site-packages/regex/_regex.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/lib/_regex.cpython-310-darwin.so' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/nltk/__init__.py:137\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrammar\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m--> 137\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjsontags\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/nltk/text.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m ConditionalFreqDist \u001b[39mas\u001b[39;00m CFD\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyConcatenation, tokenwrap\n\u001b[1;32m     32\u001b[0m ConcordanceLine \u001b[39m=\u001b[39m namedtuple(\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mConcordanceLine\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moffset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mleft_print\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mright_print\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n\u001b[0;32m---> 65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcasual\u001b[39;00m \u001b[39mimport\u001b[39;00m TweetTokenizer, casual_tokenize\n\u001b[1;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdestructive\u001b[39;00m \u001b[39mimport\u001b[39;00m NLTKWordTokenizer\n\u001b[1;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegality_principle\u001b[39;00m \u001b[39mimport\u001b[39;00m LegalitySyllableTokenizer\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/nltk/tokenize/casual.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhtml\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m---> 49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mregex\u001b[39;00m  \u001b[39m# https://github.com/nltk/nltk/issues/2409\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m TokenizerI\n\u001b[1;32m     53\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# The following strings are components in the regular expression\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# that is used for tokenizing. It's important that phone_number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m# This particular element is used in a couple ways, so we define it\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# with a name:\u001b[39;00m\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/regex/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mregex\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m regex\n\u001b[1;32m      3\u001b[0m __all__ \u001b[39m=\u001b[39m regex\u001b[39m.\u001b[39m__all__\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/regex/regex.py:421\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[39mreturn\u001b[39;00m r\n\u001b[1;32m    418\u001b[0m \u001b[39m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[39m# Internals.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mregex\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_regex_core\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_regex_core\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mregex\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_regex\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_regex\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthreading\u001b[39;00m \u001b[39mimport\u001b[39;00m RLock \u001b[39mas\u001b[39;00m _RLock\n",
      "File \u001b[0;32m~/tac/tac_venv/lib/python3.10/site-packages/regex/_regex_core.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39municodedata\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mregex\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_regex\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_regex\u001b[39;00m\n\u001b[1;32m     23\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mASCII\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBESTMATCH\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mD\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDEBUG\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mENHANCEMATCH\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFULLCASE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mIGNORECASE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLOCALE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMULTILINE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mPOSIX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mREVERSE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDOTALL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTEMPLATE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mU\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUNICODE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mV0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERSION0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mV1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERSION1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWORD\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERBOSE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mScanner\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRegexFlag\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[39m# The regex exception.\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/anoukchapuis/tac/tac_venv/lib/python3.10/site-packages/regex/_regex.cpython-310-darwin.so, 0x0002): tried: '/Users/anoukchapuis/tac/tac_venv/lib/python3.10/site-packages/regex/_regex.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/lib/_regex.cpython-310-darwin.so' (no such file)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer une une liste de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Récupération du contenu du fichier\n",
    "path = \"../data/all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path) as f:\n",
    "    text = f.read()[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "words = nltk.wordpunct_tokenize(text)\n",
    "print(f\"{len(words)} words found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminer les stopwords et les termes non alphabétiques\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les mots les plus fréquents et en faire un plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(kept)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot: les n mots les plus fréquents\n",
    "n = 10\n",
    "fdist.plot(n, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détecter les Hapax (mots qui n'apparaissent qu'une fois dans le corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.hapaxes()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver les mots les plus longs du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 30\n",
    "sorted(voc, key=len, reverse=True)[:n]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c0f851c074e63101fa943bb20d529c271884474fc3cda75dc22d6247f049de0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
